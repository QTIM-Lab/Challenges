# Terms and Conditions

The most important rule:

## Rule One
Cheating will not be tolerated.

## Other Rules, Terms, and Conditions
By participating in this Challenge, each participant agrees to the following: 

### Participants 
Participation in this Challenge acknowledges the educational and community-building nature of the Challenge and commits participants to conduct consistent with this spirit for the advancement of the medical imaging research community. 

For more background information on the expected conduct of participants, see this article  for a discussion of lessons learned from the LUNGx Challenge, which was sponsored by SPIE, AAPM, and NCI. 

Anonymous participation is not allowed. 

Participants from the same research group, company, or collaboration are required to participate as a team, i.e., form a team within the Challenge platform.  

Individual participants should form a single-user team.  

Team size is limited to 8 participants. 

Participants may only join one team. 

Entry by commercial entities is permitted but must be disclosed. 

All participants must attest that they are not directly affiliated with the labs of any of the Challenge organizers or major contributors. No conflict of interest may exist for any team to be considered in the final ranking as per [the MIDRC Grand Challenge conflict Policy ](hhttps://docs.google.com/document/d/11K2AWE1Nobx1iH4nwdBy5lMe9AmvCbKf/edit?usp=sharing&ouid=105455435698876009846&rtpof=true&sd=truettps://)

Registration after the deadline will not be considered for the Challenge (see ‘Important dates’ at the bottom of the “Overview” page). 

### AI/ML Methods 
Participants are strongly encouraged to agree to MIDRC making their code and trained model(s), including weights, publicly available after completion of the Challenge.  In order for a participating team to win cash prizes, it is a requirement to allow MIDRC to make the team's code and trained model weights publicly available. 

As part of the registration process, participants will select one of two options: 
* Upon Challenge completion, our team agrees that our trained model(s) and Docker submission(s) WILL be made public by MIDRC. 
* Our team wishes to participate in the Challenge, but we do NOT wish for our submission(s) and trained model(s) to be made public by MIDRC.  
Note: If the top 2 performing teams decide they'd like MIDRC support for going through the FDA regulatory process of their model (see the section on performance evaluation), this will preclude any requirement of making code publicly available and any publication of results by the Challenge organizers will be done in such a way as not to harm any potential commercialization prospects. 

Descriptions of participants’ methods and results may become part of presentations, publications, and subsequent analyses derived from the Challenge (with proper attribution to the participants) at the discretion of the organizers. While methods and results may become part of Challenge reports and publications, participants may choose not to disclose their identity and remain anonymous for the purpose of these reports. 

Only fully automated methods are acceptable for the Challenge. It is not possible, and not allowed, to submit manual annotations or interactive methods. 

Using transfer learning/fine-tuning of models pretrained on general-purpose datasets (e.g., ImageNet) is allowed. 

### Submissions to the Challenge 
Once participants make a submission within the test phase of the Challenge, they will be considered fully vested in the challenge, so that their performance results will become part of any presentations, publications, or subsequent analyses derived from the Challenge at the discretion of the organizers. Withdrawal at this point is not allowed but participants can choose to have their results reported anonymously in these presentations and publications. 

For submissions in the test phase, participants will be required to disclose a description of their methods and training data used. Without this, a submission will be considered invalid. In other words, a description of the method/model (plain text or Word file) needs to be included in your zip archive submission in order for a submission to be considered a valid submission, i.e., for its performance to be reported back and to be part of the Challenge.  

Each Challenge phase has a maximum number of submissions allowed per team (see the “Challenge Details” page). Submissions that result in errors flagged by the Challenge platform will be labeled “Failed” and do not count towards the maximum number of submissions allowed. After the maximum number of submissions for a team is reached, the Challenge system will not accept further submissions to the applicable Challenge phase. 

Performance Evaluation, Ranking, Prizes, and Participant Credit
Performance on the test dataset, i.e., performance in the test phase of the Challenge, will be used to rank submissions and determine the Challenge placement of participating teams.

The primary performance metric will be used to rank submissions. 

Raking of submissions will be performed using the value for the primary performance metric, without taking into account statistical significance of any differences in performance among submissions. Thus, it is not required to demonstrate a statistically significantly better performance than other submissions to "win" the Challenge.  

A secondary performance metric will be used to break ties, if needed. 

The highest-performing submission of a participating team will determine the team's ranking within the Challenge.

The top 2 performing teams will have the opportunity to receive support from MIDRC through the FDA regulatory process of their method through the evaluation of their method using the MIDRC sequestered (non-public) Data Commons, provided that their best submission substantially, and statistically significantly, outperforms random guessing.

Cash prizes will be awarded to the top 7 teams as outlined on the "Overview" page, provided that these teams agree to MIDRC making their code and trained models public and that their best submission substantially, and statistically significantly, outperforms random guessing. The top 2 teams do not need to make their code/model public if they want to take this code/model through the regulatory process with MIDRC support. 

All participants with a valid submission in the test phase will receive contributor credit on Challenge publications, acknowledgement at AAPM and RSNA 2024 Annual Meetings as well as on midrc.org.

In the validation phase, the performance of your submission will automatically be put on the Leaderboard. There is no Leaderboard in the test phase. 

See the "Challenge Details" page under the "Learn the Details" tab for the performance metrics used in this Challenge.

### Help
Technical help will be provided by Challenge organizers as much as possible if problems arise during the submission process. However, help to teams in the validation and test phases of the Challenge will be limited and may not always be possible. Teams that did not participate in the practice submissions during the training phase will receive the lowest priority and may not receive any help. 

It is imperative to read the “Challenge Details” page (link in the menu on the left) and we also suggest that you visit the “Tutorials” page (link in menu on the left) prior to attempting to make any submissions. 

The “Forum” tab should be used to communicate all questions or issues regarding the Challenge.